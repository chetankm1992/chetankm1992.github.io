<!DOCTYPE html>
<!--[if lt IE 8 ]><html class="no-js ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="no-js ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 8)|!(IE)]><!--><html class="no-js" lang="en"> <!--<![endif]-->
<head>

   <!--- Basic Page Needs
   ================================================== -->
   <meta charset="utf-8">
	<title>Chetan Kumar</title>
	<meta name="description" content="">
	<meta name="author" content="">

   <!-- Mobile Specific Metas
   ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
    ================================================== -->
   <link rel="stylesheet" href="css/default.css">
	<link rel="stylesheet" href="css/layout.css">
   <link rel="stylesheet" href="css/media-queries.css">
   <link rel="stylesheet" href="css/magnific-popup.css">

   <!-- Script
   ================================================== -->
	<script src="js/modernizr.js"></script>

   <!-- Favicons
	================================================== -->
	<link rel="shortcut icon" href="favicon.png" >

</head>

<style type="text/css">
	.p_style{
		 margin-bottom: 0px !important;
       text-align: justify;
	}
   pre code{
          background-color: #eee;
          border: 1px solid #999;
          display: block;
          padding-left: 5px;
          font-size: 12px;
   }
</style>

<body>

    <!-- About Section
   ================================================== -->
   <section id="about">
	<nav id="nav-wrap">

         <a class="mobile-btn" href="#nav-wrap" title="Show navigation">Show navigation</a>
	      <a class="mobile-btn" href="#" title="Hide navigation">Hide navigation</a>

         <ul id="nav" class="nav">
            <!-- <li class="current"><a class="smoothscroll" href="#home">Home</a></li> -->
            <li><a href="http://chetan-kumar.com">Home</a></li>
			<li><a href="http://chetan-kumar.com/research">Research</a></li>
			<li><a href="http://chetan-kumar.com/blog">Blog</a></li>
			
            <!--<li><a class="smoothscroll" href="#about">About</a></li>
            <li><a class="smoothscroll" href="#education">Education</a></li>
            <li><a class="smoothscroll" href="#experience">Experience</a></li>
			<li><a class="smoothscroll" href="#skills">Skills</a></li>
			<li><a class="smoothscroll" href="#projects">Projects</a></li>
			<li><a class="smoothscroll" href="#publication">Publication</a></li>
			<li><a class="smoothscroll" href="#services">Services</a></li>-->
         </ul> <!-- end #nav -->

    </nav> <!-- end #nav-wrap -->

 </section> <!-- About Section End-->

 <!-- Start ----- AAAI 2020 Paper -->
       <!-- About Section
   ================================================== -->
   <section id="about" style="padding-top:25px;">
      <div class="row">
         <div class="twelve columns">	
            <h2 style="text-align: center;">Skeleton Based Action Recognition using Convolutional Neural Network</h2>
			<p class="p_style">I have worked on Skeleton Based Action Recognition using Kinect V2 to determine the difference between large bechmark datasets such as <a href="http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html">PKU-MMD</a> and <a href="http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html">SBU-Kinect</a> dataset. First I have collected data using Kinect V2 using Matlab and then used a <a href="https://arxiv.org/abs/1804.06055">Convolutional Neural Network (CNN)</a> model for prediction. In this post, I will walk you with all the steps I followed and methods I used for this piece of work.</p>
			<img src="images/kinect_blog/Kinect_V2.jpg" alt="Kinect V2">
			[<a href="https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwj39vLT-8LdAhUNON8KHYDVA5AQjRx6BAgBEAU&url=https%3A%2F%2Fwww.ignatiuz.com%2Fblog%2Fnet%2Fkinect-v2-for-windows%2F&psig=AOvVaw2LSFOlh2FstmJjEtkZKCr-&ust=1537305510490132">Source</a>]
			
			<h3>1) Setting up Kinect sensor and checking that it is working properly</h3>
            <p class="p_style">I assume that you can connect Kinect device to power source and can connect to PC using USB cable. Now, you need to download <a href="https://www.microsoft.com/en-us/download/details.aspx?id=44561">Microsoft Kinect SDK 2.0 and install it.</a></p>
            <p class="p_style">When you successfully install Kinect SDK then you open find it with name "Kinect Studio v2.0" in your search bar and when you open it then it very left corner there will be icon which will display "connect to server" if you hover, you click it and you can see the ouput of Kinect cameras at right hand side.</p>

            <img src="images/kinect_blog/kinect_studio.jpg" alt="Kinect Studio Interface">
            [<a href="https://www.google.com/search?safe=off&rlz=1C1CHBF_enUS721US721&biw=1280&bih=584&tbm=isch&sa=1&ei=KBKXW8LmJ8ub_QbF2I-4BA&q=kinect+studio+v2&oq=kinect+studio&gs_l=img.3.1.0j0i24l9.1454.5006..6076...0.0..0.444.1693.6j4j1j0j1......1....1..gws-wiz-img.......35i39j0i67j0i8i30j0i10i24.AZ3uz6lk3LM#imgrc=jIra89KS7p5rdM:">Source</a>]
			
			<h3>2) Connecting Kinect to Matlab</h3>
			  <p class="p_style">There are number of steps I followed to get data from Kinect in Matlab.</p>
			  <p class="p_style">i) First is installation of <span style="color: royalblue">"Image Acquisition Toolbox"</span> which you can easily install using Matlab Add-Ons option in Home tab. You can easily install as trial version if you don't want subscription.</p>
			  <p class="p_style">ii) Now you need to install <span style="color: royalblue">"Image Acquisition Toolbox Support Package for Kinect for Windows Sensor"</span> which can be also be installed using Add-Ons in Matlab.
			  <p class="p_style">iii) Now when you run <span style="color: royalblue">"imaqhwinfo"</span> command in Matlab command prompt, if everthing is installed properly then it will display Kinect as Installed Adaptors. When you get this then you are all set for getting data from Kinect in Matlab.</p>
			  <br/>

			  <h3>2) Getting Data into Matlab</h3>
			  <p class="p_style">There is good source <a target="_blank" href="https://www.mathworks.com/help/supportpkg/kinectforwindowsruntime/examples/plot-skeletons-with-the-kinect-v2.html">Skeleton Viewer for Kinect V2 Skeletal Data</a> to follow inorder to see what sort of data you can get by Kinect and how it is stored and you can draw skeleton on top of RGB image.</p>
			  <p class="p_style">After running the code given in Matlab link then you open the metadata variable in Maltab and it will contain multiple variables. If you check <span style="color: royalblue">"IsBodyTracked"</span>, it will give you the information that anybody has been tracked or not. It can track six person at a time and if you notice there will be values like [0,0,0,0,0,0] shows no one is tracked and if values like [0,0,0,1,0,1] means two bodies are tracked. The position of 1's doesn't matter here.</p>
			  <p class="p_style">Now, if you see <span style="color: royalblue">"JointPositions"</span> variable, it will give you x,y,z coordinate values in meters of each body joint (Kinect V2 can recognize 25 body joints). </p>
			  <img src="images/kinect_blog/joints.gif" alt="Skeleton Joints">
			  [<a href="https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwiujOSg_MLdAhUChOAKHRNIDjsQjRx6BAgBEAU&url=http%3A%2F%2Fvodacek.zvb.cz%2Farchiv%2F724.html&psig=AOvVaw18jR2Ua79DIleUUw25zpXc&ust=1537305726613590">Source</a>]

			  <h3>3) Storing Data</h3>
			  <p class="p_style">Our goal for this projects was to detect and recognize interaction between 2 persons. First I follow the same procedure as in <a target="_blank" href="https://www.mathworks.com/help/supportpkg/kinectforwindowsruntime/examples/plot-skeletons-with-the-kinect-v2.html"></a> to connect Kinect and collect 200 frames for each color and depth video objects.</p>
			  <p class="p_style">Then I save metadata, colorImg inorder to draw the skeleton representaton on the colorImg (RGB data) by using metadata(depth data) as shown in below figure.</p>
			  <img src="images/kinect_blog/skeleton_representation.jpg" alt="Skeleton Representation" />
			  <p>Code snippet to get and save data. I prefer making a dedicated folder to store this data so just to have a clean storage. Such as I use kinect_data folder to save data.</p>
			  
			  <pre>
                     <code>
data = metadata; %making copy of data     
m = 1; %iterates when a body is tracked in the frame
clear data_joints_values %clearing the variable 
for n = 1:framesPerTrig %loop runs for number of frames (e.g, 200)
    currentframeMetadata = data(n);
    %we first check if anybody is tracked
    currentframeanyBodiesTracked = any(currentframeMetadata.IsBodyTracked ~= 0);
    %currenttrackedBodies = find(currentframeMetadata.IsBodyTracked);
    if currentframeanyBodiesTracked == 1 %if frame is not empty
        data_color_joints = data(n).JointPositions; %we get data for each frame
        i = 1; k = 2; l = 3; %iteration for x,y,z positions for first person repectively
        ii = 76; kk = 77; ll = 78; %iteration for x,y,z positions for second person repectively
        for j = 1:25
            %saving data for first person
            data_joints_values(m,i) = data_color_joints(j,1,trackedBodies(1));
            data_joints_values(m,k) = data_color_joints(j,2,trackedBodies(1));
            data_joints_values(m,l) = data_color_joints(j,3,trackedBodies(1));
            
            %saving data for second person
            data_joints_values(m,ii) = data_color_joints(j,1,trackedBodies(2));
            data_joints_values(m,kk) = data_color_joints(j,2,trackedBodies(2));
            data_joints_values(m,ll) = data_color_joints(j,3,trackedBodies(2));
            i = i+3; k = k+3; l = l+3;
            ii = ii + 3; kk = kk + 3; ll  = ll + 3;
        end
            m = m+1;
     end
end
data_save = data_joints_values; %variable to save data
dlmwrite(kinect_data\data.txt, ' ') %saving data in file where each value is separated by space
                     </code>
                  </pre>

			  <h3>Training and Testing CNN Model</h3>
                <p class="p_style">I have followed a paper named <a target="_blank" href="https://arxiv.org/abs/1804.06055">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and
Detection with Hierarchical Aggregation</a> and also used its source code <a target="_blank" href="https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition">Github</a> to train UMassd-Kinect dataset.</p>
				<p class="p_style">Howere this code is trained on SBU-Kinect dataset which consists of 15 body joint coordinates while UMassd-Kinect contains 25 body joints so I updated their model to train on 25 body joints. I also updated this code so that it can get data from Matlab at runtime and recognize the action in real time.</p>
				<p class="p_style">Purpose of real time recognition is to present a study which can differentiate between synthetic data (taken in some laboratory or controlled lab environment) and non-synthetic data (real time without considering any lighting, illuminations, background, etc settings). This model seems to give good results on relatively small dataset.</p>

				<h3>Code</h3>
				<p class="p_style">Full code containing matalb and python files is uploaded on github <a target="_blank" href="https://github.com/chetankm1992/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition">URL.</a></p>
				<ol>
					<li>Live_Skeleton_Action_Recognition.ipynb</li>
					<p style="text-align: justify;">After setting Kinect sensor and connecting it with Matalb code as described in above (1) and (2) steps, you can you use this code for live demo. This code contains the pre-training model (my_model.h5) on UMassd-Kinect dataset and it also calls Matlab function "live_demo.m" to get the data and matlab function initiates Kinect sensor to get the data at run time and sends it back to jupyter file. Then model gives the prediction results.</p>
					<li>Co-occurrence Feature Learning for Action Recognition for Multi-View Dataset containing 25 (3D) Body Joints.ipynb</li>
					<p style="text-align: justify; margin-bottom: 0px;">However, if you want to do the training by yourself then you can use this code to train the model from sketch. It uses data given under "data" folder which contains five classes (hand shake, point finger, giving copy, hug and tap back) taken by 5 different subjects and considered three different angles and backgrounds with variations. Dataset is also provided with train and test split. You can also use anyother dataset for training and testing the model.</p>
				</ol>

                  <p>Continue</p>
			  
            </div> <!-- end row -->
      </div> <!-- end .main-col -->
   </section> <!-- About Section End-->
<!-- End ----- AAAI 2020 Paper -->
      <div class="row">

         <!--<div class="three columns">

            <img class="profile-pic"  src="images/profilepic.jpg" alt="" />

         </div>-->

         <!--<div class="nine columns main-col">-->
         <div class="twelve columns">	

            <h2 style="text-align: center;">Skeleton Based Action Recognition using Convolutional Neural Network</h2>

            <!-- <p style="text-align:justify; margin:0px !important">I am ambitious Masters student in Data Science at University of Massachusetts, US. My research foucses on Deep Learning, Computer Vision, Transfer Learning and Data Visualization. I am working on my Masters project on Clothing Detection using Machine Learning under the supervision of <a href="http://www.cis.umassd.edu/~mshao/">Dr.Ming Shao.</a></p>
			<p style="text-align:justify; margin:0px !important">I have completed my Masters and Bachelors in Computer Science from Shaheed Zulfikar Ali Bhutto Institute of Science and Technology (SZABIST) and I did my Masters Thesis on Sentiment Analysis of Roman Urdu Text.</p>
			
            <div class="row" style="margin-top:20px;">

               <div class="columns contact-details">

                  <p class="address">
						   <span style="font: 16px/0px 'opensans-bold', sans-serif; color:white;">Neural and Intelligent System Lab</span><br>
						   <span style="font: 16px/0px 'opensans-bold', sans-serif; color:white;">University of Massachusetts</span><br>
						   <span style="font: 16px/0px 'opensans-bold', sans-serif; color:white;">(508)441-7120</span><br>
						   <span style="font: 16px/0px 'opensans-bold', sans-serif; color:white;">ckumar@umassd.edu</span>
					   </p>

               </div>

               <div class="columns download">
                  <p style="margin:0px !important;">
                     <a href="resume/Chetan-Resume.pdf" class="button" style="margin: 0px !important;"><i class="fa fa-download"></i>Download Resume</a>
                  </p>
				<ul class="social">
					   <li><a href="https://www.linkedin.com/in/chetankumar92/"><i class="fa fa-linkedin"></i></a></li>
					   <li><a href="https://github.com/chetankm1992"><i class="fa fa-github"></i></a></li>
				</ul>
               </div>-->

            </div> <!-- end row -->
		<!-- <ul class="social">
               <li><a href="#"><i class="fa fa-facebook"></i></a></li>
               <li><a href="#"><i class="fa fa-twitter"></i></a></li>
               <li><a href="#"><i class="fa fa-google-plus"></i></a></li>
               <li><a href="#"><i class="fa fa-linkedin"></i></a></li>
               <li><a href="#"><i class="fa fa-instagram"></i></a></li>
               <li><a href="#"><i class="fa fa-dribbble"></i></a></li>
               <li><a href="#"><i class="fa fa-skype"></i></a></li>
		</ul> -->
      </div> <!-- end .main-col -->

      <!--</div>-->

   </section> <!-- About Section End-->


   <!-- Resume Section
   ================================================== -->
   <section id="kinect_blog">

      <!-- kinect_blog
      ----------------------------------------------- -->
      <div class="row kinect_blog">

         <div class="three columns header-col">
            <h1><span>Introduction</span></h1>
         </div>

         <div class="nine columns main-col">

            <div class="row item">

               <div class="twelve columns">

                  <!--<h3>University of Massachusetts, Dartmouth US</h3>-->
                  <p class="p_style">I have worked on Skeleton Based Action Recognition using Kinect V2 to determine the difference between large bechmark datasets such as <a href="http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html">PKU-MMD</a> and <a href="http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html">SBU-Kinect</a> dataset. First I have collected data using Kinect V2 using Matlab and then used a <a href="https://arxiv.org/abs/1804.06055">Convolutional Neural Network (CNN)</a> model for prediction. In this post, I will walk you with all the steps I followed and methods I used for this piece of work.</p>
                  <img src="images/kinect_blog/Kinect_V2.jpg" alt="Kinect V2">
                  [<a href="https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwj39vLT-8LdAhUNON8KHYDVA5AQjRx6BAgBEAU&url=https%3A%2F%2Fwww.ignatiuz.com%2Fblog%2Fnet%2Fkinect-v2-for-windows%2F&psig=AOvVaw2LSFOlh2FstmJjEtkZKCr-&ust=1537305510490132">Source</a>]


                  <!-- <p>
                  Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Aenean commodo ligula eget dolor. Aenean massa.
                  Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Donec quam felis,
                  ultricies nec, pellentesque eu, pretium quis, sem. Nulla consequat massa quis enim.
                  Donec pede justo, fringilla vel, aliquet nec, vulputate eget, arcu. Nullam dictum felis eu pede mollis pretium.
                  </p> -->

               </div>

            </div> <!-- item end -->

            <div class="row item">

               <div class="twelve columns">

                  <h3>1) Setting up Kinect sensor and checking that it is working properly</h3>
                  <p class="p_style">I assume that you can connect Kinect device to power source and can connect to PC using USB cable. Now, you need to download <a href="https://www.microsoft.com/en-us/download/details.aspx?id=44561">Microsoft Kinect SDK 2.0 and install it.</a></p>
                  <p class="p_style">When you successfully install Kinect SDK then you open find it with name "Kinect Studio v2.0" in your search bar and when you open it then it very left corner there will be icon which will display "connect to server" if you hover, you click it and you can see the ouput of Kinect cameras at right hand side.</p>

                  <img src="images/kinect_blog/kinect_studio.jpg" alt="Kinect Studio Interface">
                  [<a href="https://www.google.com/search?safe=off&rlz=1C1CHBF_enUS721US721&biw=1280&bih=584&tbm=isch&sa=1&ei=KBKXW8LmJ8ub_QbF2I-4BA&q=kinect+studio+v2&oq=kinect+studio&gs_l=img.3.1.0j0i24l9.1454.5006..6076...0.0..0.444.1693.6j4j1j0j1......1....1..gws-wiz-img.......35i39j0i67j0i8i30j0i10i24.AZ3uz6lk3LM#imgrc=jIra89KS7p5rdM:">Source</a>]

                  <h3>2) Connecting Kinect to Matlab</h3>
                  <p class="p_style">There are number of steps I followed to get data from Kinect in Matlab.</p>
                  <p class="p_style">i) First is installation of <span style="color: royalblue">"Image Acquisition Toolbox"</span> which you can easily install using Matlab Add-Ons option in Home tab. You can easily install as trial version if you don't want subscription.</p>
                  <p class="p_style">ii) Now you need to install <span style="color: royalblue">"Image Acquisition Toolbox Support Package for Kinect for Windows Sensor"</span> which can be also be installed using Add-Ons in Matlab.
                  <p class="p_style">iii) Now when you run <span style="color: royalblue">"imaqhwinfo"</span> command in Matlab command prompt, if everthing is installed properly then it will display Kinect as Installed Adaptors. When you get this then you are all set for getting data from Kinect in Matlab.</p>
                  <br/>

                  <h3>2) Getting Data into Matlab</h3>
                  <p class="p_style">There is good source <a target="_blank" href="https://www.mathworks.com/help/supportpkg/kinectforwindowsruntime/examples/plot-skeletons-with-the-kinect-v2.html">Skeleton Viewer for Kinect V2 Skeletal Data</a> to follow inorder to see what sort of data you can get by Kinect and how it is stored and you can draw skeleton on top of RGB image.</p>
                  <p class="p_style">After running the code given in Matlab link then you open the metadata variable in Maltab and it will contain multiple variables. If you check <span style="color: royalblue">"IsBodyTracked"</span>, it will give you the information that anybody has been tracked or not. It can track six person at a time and if you notice there will be values like [0,0,0,0,0,0] shows no one is tracked and if values like [0,0,0,1,0,1] means two bodies are tracked. The position of 1's doesn't matter here.</p>
                  <p class="p_style">Now, if you see <span style="color: royalblue">"JointPositions"</span> variable, it will give you x,y,z coordinate values in meters of each body joint (Kinect V2 can recognize 25 body joints). </p>
                  <img src="images/kinect_blog/joints.gif" alt="Skeleton Joints">
                  [<a href="https://www.google.com/url?sa=i&rct=j&q=&esrc=s&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwiujOSg_MLdAhUChOAKHRNIDjsQjRx6BAgBEAU&url=http%3A%2F%2Fvodacek.zvb.cz%2Farchiv%2F724.html&psig=AOvVaw18jR2Ua79DIleUUw25zpXc&ust=1537305726613590">Source</a>]

                  <h3>3) Storing Data</h3>
                  <p class="p_style">Our goal for this projects was to detect and recognize interaction between 2 persons. First I follow the same procedure as in <a target="_blank" href="https://www.mathworks.com/help/supportpkg/kinectforwindowsruntime/examples/plot-skeletons-with-the-kinect-v2.html"></a> to connect Kinect and collect 200 frames for each color and depth video objects.</p>
                  <p class="p_style">Then I save metadata, colorImg inorder to draw the skeleton representaton on the colorImg (RGB data) by using metadata(depth data) as shown in below figure.</p>
                  <img src="images/kinect_blog/skeleton_representation.jpg" alt="Skeleton Representation" />
                  <p>Code snippet to get and save data. I prefer making a dedicated folder to store this data so just to have a clean storage. Such as I use kinect_data folder to save data.</p>
                  <pre><code>
[colorImg] = getdata(colorVid);
[~, ~, metadata] = getdata(depthVid);
save(kinect_data\metadata.mat, 'colorImg');
save(kinect_data\colorImg.mat, 'metadata');
                  </code></pre>
                  <br/>
                  <h5>Storing Joint Position (3D)</h5>
                  <p class="p_style">Now, we need to store joints positions values in a specific format which large scale skeleton datasets such as <a target="_blank" href="http://www3.cs.stonybrook.edu/~kyun/research/kinect_interaction/index.html">SBU-Kinect</a>, <a target="_blank" href="http://www.icst.pku.edu.cn/struct/Projects/PKUMMD.html">PKU-MMD</a>, use to store data inorder to make it viable to be used in any machine learning methods without need of changing the data format or orientation. So we save video frames in rows while columns contains values of each joint. So, we have 3D(x,y,z) values of each joint and we have 25 joints for one person, so first 75 columns contains values for one person likewise other 75 columns contain values for second person and intotal we have 150 columns while we fixed frames to 200 so we have 200x175 dimension of data. It can be better seen in the following figure.</p>
                  <img src=""/>
                  <img src="images/kinect_blog/skeleton_data_save_format.png" alt="Skeleton Representation"><br/>
                  [<a href="https://www.semanticscholar.org/paper/Co-occurrence-Feature-Learning-from-Skeleton-Data-Li-Zhong/1b10e59adfa3f0f7748f48e2e64e54db2a5362d3">Source</a>]
                  <p class="p_style">I code like this to save data in text files.</p>
                  <pre>
                     <code>
data = metadata; %making copy of data     
m = 1; %iterates when a body is tracked in the frame
clear data_joints_values %clearing the variable 
for n = 1:framesPerTrig %loop runs for number of frames (e.g, 200)
    currentframeMetadata = data(n);
    %we first check if anybody is tracked
    currentframeanyBodiesTracked = any(currentframeMetadata.IsBodyTracked ~= 0);
    %currenttrackedBodies = find(currentframeMetadata.IsBodyTracked);
    if currentframeanyBodiesTracked == 1 %if frame is not empty
        data_color_joints = data(n).JointPositions; %we get data for each frame
        i = 1; k = 2; l = 3; %iteration for x,y,z positions for first person repectively
        ii = 76; kk = 77; ll = 78; %iteration for x,y,z positions for second person repectively
        for j = 1:25
            %saving data for first person
            data_joints_values(m,i) = data_color_joints(j,1,trackedBodies(1));
            data_joints_values(m,k) = data_color_joints(j,2,trackedBodies(1));
            data_joints_values(m,l) = data_color_joints(j,3,trackedBodies(1));
            
            %saving data for second person
            data_joints_values(m,ii) = data_color_joints(j,1,trackedBodies(2));
            data_joints_values(m,kk) = data_color_joints(j,2,trackedBodies(2));
            data_joints_values(m,ll) = data_color_joints(j,3,trackedBodies(2));
            i = i+3; k = k+3; l = l+3;
            ii = ii + 3; kk = kk + 3; ll  = ll + 3;
        end
            m = m+1;
     end
end
data_save = data_joints_values; %variable to save data
dlmwrite(kinect_data\data.txt, ' ') %saving data in file where each value is separated by space
                     </code>
                  </pre>

                <h3>Training and Testing CNN Model</h3>
                <p class="p_style">I have followed a paper named <a target="_blank" href="https://arxiv.org/abs/1804.06055">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and
Detection with Hierarchical Aggregation</a> and also used its source code <a target="_blank" href="https://github.com/fandulu/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition">Github</a> to train UMassd-Kinect dataset.</p>
				<p class="p_style">Howere this code is trained on SBU-Kinect dataset which consists of 15 body joint coordinates while UMassd-Kinect contains 25 body joints so I updated their model to train on 25 body joints. I also updated this code so that it can get data from Matlab at runtime and recognize the action in real time.</p>
				<p class="p_style">Purpose of real time recognition is to present a study which can differentiate between synthetic data (taken in some laboratory or controlled lab environment) and non-synthetic data (real time without considering any lighting, illuminations, background, etc settings). This model seems to give good results on relatively small dataset.</p>

				<h3>Code</h3>
				<p class="p_style">Full code containing matalb and python files is uploaded on github <a target="_blank" href="https://github.com/chetankm1992/Keras-for-Co-occurrence-Feature-Learning-from-Skeleton-Data-for-Action-Recognition">URL.</a></p>
				<ol>
					<li>Live_Skeleton_Action_Recognition.ipynb</li>
					<p style="text-align: justify;">After setting Kinect sensor and connecting it with Matalb code as described in above (1) and (2) steps, you can you use this code for live demo. This code contains the pre-training model (my_model.h5) on UMassd-Kinect dataset and it also calls Matlab function "live_demo.m" to get the data and matlab function initiates Kinect sensor to get the data at run time and sends it back to jupyter file. Then model gives the prediction results.</p>
					<li>Co-occurrence Feature Learning for Action Recognition for Multi-View Dataset containing 25 (3D) Body Joints.ipynb</li>
					<p style="text-align: justify; margin-bottom: 0px;">However, if you want to do the training by yourself then you can use this code to train the model from sketch. It uses data given under "data" folder which contains five classes (hand shake, point finger, giving copy, hug and tap back) taken by 5 different subjects and considered three different angles and backgrounds with variations. Dataset is also provided with train and test split. You can also use anyother dataset for training and testing the model.</p>
				</ol>

                  <p>Continue</p>
               </div>

            </div> <!-- item end -->

        	


         </div> <!-- main-col end -->

      </div> <!-- End Education -->


   </section> <!-- End Education section -->
  

   <!-- footer
   ================================================== -->
   <footer>

      <div class="row">

         <div class="twelve columns">

            <ul class="social-links">
               <!-- <li><a href="#"><i class="fa fa-facebook"></i></a></li> -->
               <!-- <li><a href="#"><i class="fa fa-twitter"></i></a></li> -->
               <!-- <li><a href="#"><i class="fa fa-google-plus"></i></a></li> -->
               <li><a href="https://www.linkedin.com/in/chetankumar92/"><i class="fa fa-linkedin"></i></a></li>
               <li><a href="https://github.com/chetankm1992"><i class="fa fa-github"></i></a></li>
               <!-- <li><a href="#"><i class="fa fa-instagram"></i></a></li> -->
               <!-- <li><a href="#"><i class="fa fa-dribbble"></i></a></li> -->
               <!-- <li><a href="#"><i class="fa fa-skype"></i></a></li> -->
            </ul>
            
         </div>

         <div id="go-top"><a class="smoothscroll" title="Back to Top" href="#about"><i class="icon-up-open"></i></a></div>

      </div>

   </footer> <!-- Footer End-->

   <!-- Java Script
   ================================================== -->
   <script src="http://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
   <script>window.jQuery || document.write('<script src="js/jquery-1.10.2.min.js"><\/script>')</script>
   <script type="text/javascript" src="js/jquery-migrate-1.2.1.min.js"></script>

   <script src="js/jquery.flexslider.js"></script>
   <script src="js/waypoints.js"></script>
   <script src="js/jquery.fittext.js"></script>
   <script src="js/magnific-popup.js"></script>
   <script src="js/init.js"></script>

</body>

</html>